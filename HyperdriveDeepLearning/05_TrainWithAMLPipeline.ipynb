{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish a Training Pipeline\n",
    "In this notebook, we will show how to automate the training/retraining of model using HyperDrive and registering best model. Once this training pipeline is published/created, it provides a REST endpoint which can be called to run this pipeline without using the Azure Machine Learning Service SDK.\n",
    "\n",
    "\n",
    "## Imports  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "import azureml\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.steps import HyperDriveStep, PythonScriptStep\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PipelineParameter\n",
    "from azureml.core.runconfig import RunConfiguration, CondaDependencies\n",
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.core.container_registry import ContainerRegistry\n",
    "from azureml.train.hyperdrive import (\n",
    "    RandomParameterSampling,\n",
    "    BanditPolicy,\n",
    "    uniform,\n",
    "    choice,\n",
    "    HyperDriveConfig,\n",
    "    PrimaryMetricGoal,\n",
    ")\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "from dotenv import set_key, get_key, find_dotenv\n",
    "from utilities import get_auth\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n",
    "\n",
    "from MetricsUtils.hpStatisticsCollection import statisticsCollector, CollectionEntry\n",
    "from MetricsUtils.storageutils import storageConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = find_dotenv(raise_error_if_not_found=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Azure ML workspace and default datastore\n",
    "Read in the the workspace created in a previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "auth = get_auth(env_path)\n",
    "ws = Workspace.from_config(auth=auth)\n",
    "print(ws.name, ws.resource_group, ws.location, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Model Hyperparameters\n",
    "We automatically tune hyperparameters by exploring the range of values defined for each hyperparameter. Here we use random sampling which randomly selects hyperparameter values from the defined search space. Random sampling allows for both discrete and continuous hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sampling = RandomParameterSampling(\n",
    "    {\n",
    "        \"learning_rate\": uniform(0.0005, 0.005),\n",
    "        \"rpn_nms_thresh\": uniform(0.3, 0.7),\n",
    "        \"anchor_sizes\": choice(\n",
    "            \"16\",\n",
    "            \"16,32\",\n",
    "            \"16,32,64\",\n",
    "            \"16,32,64,128\",\n",
    "            \"16,32,64,128,256\",\n",
    "            \"16,32,64,128,256,512\",\n",
    "        ),\n",
    "        \"anchor_aspect_ratios\": choice(\n",
    "            \"0.25\", \"0.25,0.5\", \"0.25,0.5,1.0\", \"0.25,0.5,1.0,2.0\"\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The num epochs and maximum total run parameters deliberately have a low default value for the speed of running. In actual application, set these to higher values (i.e. num_epochs = 10, max_total_runs = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# number of epochs\n",
    "num_epochs = 1\n",
    "\n",
    "# max total runs for hyperdrive\n",
    "max_total_runs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to specify a maximum duration for the tuning experiment by setting `max_duration_minutes`. If both of these parameters are specified, any remaining runs are terminated once `max_duration_minutes` have passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will terminate poorly performing runs automatically with bandit early termination policy which is based on slack factor and evaluation interval. The policy terminates any run where the primary metric is not within the specified slack factor with respect to the best performing training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_termination_policy = BanditPolicy(\n",
    "    slack_factor=0.15, evaluation_interval=2, delay_evaluation=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an estimator <a id='estimator'></a>\n",
    "Create an estimator that specifies the location of the script, sets up its fixed parameters, including the location of the data, the compute target, and specifies the packages needed to run the script. It may take a while to prepare the run environment the first time an estimator is used, but that environment will be used until the list of packages is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = get_key(env_path, 'cluster_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"STANDARD_NC6\", max_nodes=8\n",
    "    )\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster.\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder = \"./torchdetect\"\n",
    "image_name = get_key(env_path, \"image_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point to an image in private ACR\n",
    "image_registry_details = ContainerRegistry()\n",
    "image_registry_details.address = get_key(env_path, \"acr_server_name\")\n",
    "image_registry_details.username = get_key(env_path, \"acr_username\")\n",
    "image_registry_details.password = get_key(env_path, \"acr_password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    source_directory=script_folder,\n",
    "    compute_target=compute_target,\n",
    "    entry_script=\"train.py\",\n",
    "    use_docker=True,\n",
    "    custom_docker_image=image_name,\n",
    "    image_registry_details=image_registry_details,\n",
    "    user_managed=True,\n",
    "    use_gpu=True,\n",
    ")\n",
    "\n",
    "estimator.run_config.environment.environment_variables[\"PYTHONPATH\"] = \"$PYTHONPATH:/cocoapi/PythonAPI/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the estimator and the configuration information together into an HyperDrive run configuration object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_config = HyperDriveConfig(\n",
    "    estimator=estimator,\n",
    "    hyperparameter_sampling=param_sampling,\n",
    "    policy=early_termination_policy,\n",
    "    primary_metric_name=\"mAP@IoU=0.50\",\n",
    "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "    max_total_runs=max_total_runs,\n",
    "    max_concurrent_runs=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Machine Learning Pipelines: Overview <a id='aml_pipeline_overview'></a>\n",
    "\n",
    "A common scenario when using machine learning components is to have a data workflow that includes the following steps:\n",
    "\n",
    "- Preparing/preprocessing a given dataset for training, followed by\n",
    "- Training a machine learning model on this data, and then\n",
    "- Deploying this trained model in a separate environment, and finally\n",
    "- Running a batch scoring task on another data set, using the trained model.\n",
    "\n",
    "Azure's Machine Learning pipelines give you a way to combine multiple steps like these into one configurable workflow, so that multiple agents/users can share and/or reuse this workflow. Machine learning pipelines thus provide a consistent, reproducible mechanism for building, evaluating, deploying, and running ML systems.\n",
    "\n",
    "To get more information about Azure machine learning pipelines, please read our [Azure Machine Learning Pipelines overview](https://aka.ms/pl-concept), or the [getting started notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb).\n",
    "\n",
    "Let's create a data reference for the raw data to be used in HyperDrive run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = DataReference(datastore=ds, data_reference_name=\"data_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = PipelineParameter(name=\"model_name\", default_value=\"torchvision_best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AML Pipeline Tuning Step \n",
    "We create a HyperDrive step in the AML pipeline to perform a search for hyperparameters. The `tune_epochs` pipeline parameter that controls the number of epochs used in tuning deliberately has a low default value for the speed of pipeline testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_step_name=\"tune_model\"\n",
    "tune_epochs = PipelineParameter(name=\"tune_epochs\", default_value=1)  # Set to 10 when running the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_step = HyperDriveStep(\n",
    "    name=tune_step_name,\n",
    "    hyperdrive_config=hyperdrive_config,\n",
    "    estimator_entry_script_arguments=[\"--data_path\", data_folder,\n",
    "                                      \"--workers\", 8,\n",
    "                                      \"--epochs\", tune_epochs,\n",
    "                                      \"--box_nms_thresh\", 0.3,\n",
    "                                      \"--box_score_thresh\", 0.10],\n",
    "    inputs=[data_folder],\n",
    "    allow_reuse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AML Pipeline Register Model Step\n",
    "This Python script step registers the best model found by the HyperDrive step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a folder for the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder = \"./registermodel\"\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile registermodel/Register_Model.py\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "from azureml.core import Run\n",
    "from azureml.pipeline.core import PipelineRun\n",
    "from azureml.pipeline.steps import HyperDriveStepRun\n",
    "import azureml.core\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"azureml.core.VERSION={}\".format(azureml.core.VERSION))\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Register the model created by\"\n",
    "                                     \" an HyperDrive step\")\n",
    "    parser.add_argument(\"--hd-step\", dest=\"hd_step\",\n",
    "                        help=\"the name of the HyperDrive step\")\n",
    "    parser.add_argument(\"--outputs\", help=\"the model file outputs directory\")\n",
    "    parser.add_argument(\"--model-name\", dest=\"model_name\",\n",
    "                        help=\"the model file base name\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    model_name = args.model_name\n",
    "    model_file = \"model_latest.pth\"\n",
    "    model_path = os.path.join(args.outputs, model_file)\n",
    "    \n",
    "    # Get the HyperDrive run.\n",
    "    run = Run.get_context()\n",
    "    print(run)\n",
    "    pipeline_run = PipelineRun(run.experiment, run.parent.id)\n",
    "    print(pipeline_run)\n",
    "    hd_step_run = HyperDriveStepRun(step_run=pipeline_run.find_step_run(args.hd_step)[0])\n",
    "    print(hd_step_run)\n",
    "    \n",
    "    # Get the best run.\n",
    "    hd_step_run.wait_for_completion(show_output=True, wait_post_processing=True)\n",
    "    best_run = hd_step_run.get_best_run_by_primary_metric()   \n",
    "    if best_run is None:\n",
    "        raise Exception(\"No best run was found\")\n",
    "    print(best_run)\n",
    "    \n",
    "    # Register the model\n",
    "    model = best_run.register_model(model_name=model_name, model_path=model_path)\n",
    "    print(\"Best Model registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating PythonScript Step for AML pipeline to register the best model. The `bm_steps_data` input pipeline data is only used to synchronize with the previous pipeline step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_step_name = \"register_model\"\n",
    "rm_run_config = RunConfiguration(conda_dependencies=CondaDependencies.create(\n",
    "    pip_packages=[\"azure-cli\", \"azureml-sdk\", \"azureml-pipeline\"]))\n",
    "rm_run_config.environment.docker.enabled = True\n",
    "rm_step = PythonScriptStep(\n",
    "    name=rm_step_name,\n",
    "    script_name=\"Register_Model.py\",\n",
    "    compute_target=compute_target,\n",
    "    source_directory=os.path.join(\".\", \"registermodel\"),\n",
    "    arguments=[\"--hd-step\", tune_step_name,\n",
    "               \"--outputs\", \"outputs\",\n",
    "               \"--model-name\", model_name],\n",
    "    runconfig=rm_run_config,\n",
    "    allow_reuse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's specify to run register model step after tune model step in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_step.run_after(tune_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create & Run a Pipeline\n",
    "When we specify the rm_step, Pipeline walks the dependency graph to include the other steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "experiment_name = \"torchvision\"\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "pipeline = Pipeline(workspace=ws, steps=[rm_step])\n",
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline before publishing it. Wait for the run to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = exp.submit(pipeline, continue_on_step_failure=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish The Pipeline \n",
    "You may read more about why to publish a pipeline and how the published pipeline can be triggered [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-ml-pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name=\"DL HyperDrive Pipeline\",\n",
    "                                      description=\"DL HyperDrive Pipeline\",\n",
    "                                      continue_on_step_failure=True)\n",
    "published_pipeline.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run published pipeline using its REST endpoint\n",
    "This step shows how to call the REST endpoint of a published pipeline to trigger the pipeline run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# aad_token = auth.get_authentication_header()\n",
    "# rest_endpoint = published_pipeline.endpoint\n",
    "# print(\"You can perform HTTP POST on URL {} to trigger this pipeline\".format(rest_endpoint))\n",
    "# response = requests.post(rest_endpoint, \n",
    "#                          headers=aad_token, \n",
    "#                          json={\"ExperimentName\": experiment_name,\n",
    "#                                \"RunSource\": \"SDK\"})\n",
    "# run_id = response.json()[\"Id\"]\n",
    "# print(run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule the published pipeline to run regularly\n",
    "This step shows how to schedule the published pipeline to run regularly. This will also submit an initial run since a starting time for the schedule is not supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule_name = \"HypeTuningDL\"\n",
    "frequency = \"Hour\"\n",
    "interval = 2\n",
    "recurrence = ScheduleRecurrence(frequency=frequency, interval=interval)\n",
    "schedule = Schedule.create(\n",
    "    workspace=ws,\n",
    "    name=schedule_name,\n",
    "    pipeline_id=published_pipeline.id,\n",
    "    experiment_name=experiment_name,\n",
    "    recurrence=recurrence,\n",
    "    description=schedule_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Out the URI \n",
    "Write the URI to the statistics tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statisticsCollector.addEntry(CollectionEntry.AKS_REALTIME_ENDPOINT,\n",
    "                             published_pipeline.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a connection string to the workspace's storage to use to save the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageConnString = \"YOUR_STORAGE_CONNECTION_STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if storageConnString == \"YOUR_STORAGE_CONNECTION_STRING\":\n",
    "    resource_group = ws.resource_group\n",
    "    stgAcctName = ws.get_details()['storageAccount'].split('/')[-1]\n",
    "    storageConnString = storageConnection.getConnectionStringWithAzCredentials(resource_group, stgAcctName)\n",
    "print(\"storage_conn_string: {}\".format(storageConnString))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the statistics collected so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statisticsCollector.uploadContent(storageConnString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now proceed to the next notebook to [delete the resources of this tutorial](06_TearDown.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TorchDetectAML]",
   "language": "python",
   "name": "conda-env-TorchDetectAML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
